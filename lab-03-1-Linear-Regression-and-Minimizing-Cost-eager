import tensorflow as tf
import numpy as np
#탠서플로우와 numpy를 불러와 실행시킨다.

tf.enable_eager_execution()
tf.__version__

import numpy as np
#실행

X = np.array([1, 2, 3])
Y = np.array([1, 2, 3])
#x,y에 데이터값 1,2,3을 입력한다.

def cost_func(W, X, Y): #cost_func함수를 만든다.
    c = 0 #c값을 0으로 초기화 시켜준다.
    for i in range(len(X)): #x길이만큼 i를 반복한다.
        c += (W * X[i] - Y[i]) ** 2 #C에 ()안의 값을 더한다
    return c / len(X)

for feed_W in np.linspace(-3, 5, num=15): #feed_W에 -3부터 5를 15개로 나누어 값을 넣는다.
    curr_cost = cost_func(feed_W, X, Y) #curr_cost에 feed_w,x,y값을 cost_func 시킨 값을 넣는다
    print("{:6.3f} | {:10.5f}".format(feed_W, curr_cost)) #값을 출력한다.


X = np.array([1, 2, 3])
Y = np.array([1, 2, 3])
#x,y에 데이터값 1,2,3을 입력한다.

def cost_func(W, X, Y):#cost_func함수를 만든다.
  hypothesis = X * W #hypothesis를 간단한 공식인 x*w로 해준다.
  return tf.reduce_mean(tf.square(hypothesis - Y))
  #hypothesis에서 Y를 빼준 값을 제곱하고 모두 더하여 평균을 정한다.

W_values = np.linspace(-3, 5, num=15)
cost_values = []
#numpy에 -3부터 5를 15개로 나누어 W_values로 값을 넣는다.

for feed_W in W_values: #feed_W값을 W_value에 넣는다.
    curr_cost = cost_func(feed_W, X, Y) #curr_cost에 feed_w,x,y값을 cost_func 시킨 값을 넣는다
    cost_values.append(curr_cost) #cost_value리스트 끝에 curr_cost값을 추가한다.
    print("{:6.3f} | {:10.5f}".format(feed_W, curr_cost)) #실행한 것을 출력한다.
    
import matplotlib.pyplot as plt #matplotlib.pyplot을 불러와 plt로 지정한다.
plt.rcParams["figure.figsize"] = (8,6)

import matplotlib.pyplot as plt
#matplotlib.pyplot을 불러와 plt로 지정한다.

plt.plot(W_values, cost_values, "b")
plt.ylabel('Cost(W)')
plt.xlabel('W')
plt.show()

tf.set_random_seed(0)  # for reproducibility
#랜덤시드를 0으로 초기화한다.
x_data = [1., 2., 3., 4.]
y_data = [1., 3., 5., 7.]
#x_data에 1,2,3,4를 넣고 y_data에 1부터 7을 넣는다.

W = tf.Variable(tf.random_normal([1], -100., 100.))

for step in range(300):#300번 반복한다.
    hypothesis = W * X #hypothesis를 간단한 공식인 x*w로 해준다.
    cost = tf.reduce_mean(tf.square(hypothesis - Y))
     #hypothesis에서 Y를 빼준 값을 제곱하고 모두 더하여 평균을 정한다.

    alpha = 0.01 #alpha에 0.01을 준다.
    gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X))
    #gradient를 cost함수를 미분시켜 만든다.
    descent = W - tf.multiply(alpha, gradient)
    #gradient와 alpha를 곱한 값을 w에서 뺀다.
    W.assign(descent) #descent를 w로 한다.
    
    if step % 10 == 0:
        print('{:5} | {:10.4f} | {:10.6f}'.format(
            step, cost.numpy(), W.numpy()[0]))
    #10번마다 확인할 수 있도록 값을 출력한다.
            
print(5.0 * W)
print(2.5 * W)
#값을 출력한다.

x_data = [1., 2., 3., 4.]
y_data = [1., 3., 5., 7.]
#x_data에 1,2,3,4를 넣고 y_data에 1부터 7을 넣는다.

W = tf.Variable([5.0])

for step in range(300):#300번 반복한다.
    hypothesis = W * X #hypothesis를 간단한 공식인 x*w로 해준다.
    cost = tf.reduce_mean(tf.square(hypothesis - Y))
     #hypothesis에서 Y를 빼준 값을 제곱하고 모두 더하여 평균을 정한다.

    alpha = 0.01 #alpha에 0.01을 준다.
    gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X))
    #gradient를 cost함수를 미분시켜 만든다.
    descent = W - tf.multiply(alpha, gradient)
    #gradient와 alpha를 곱한 값을 w에서 뺀다.
    W.assign(descent) #descent를 w로 한다.
